# 5-3. 트리의 앙상블

### 🏎️ 정형 데이터와 비정형 데이터

- 정형 데이터 (structured data) : 어떠한 구조로 되어 있는 데이터로 csv, 데이터베이스, 엑셀 등에 저장하기 쉽다.
- 비정형 데이터 (unstructured data) : 데이터베이스나 엑셀로 표현하기 어려운 데이터. 텍스트 데이터, 사진, 음악 등.

**앙상블 학습 (ensemble learning)**

정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘. 대부분 결정 트리를 기반으로 만들어져 있다. 

더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘. 

** 비정형 데이터는 신경망 알고리즘을 주로 사용

### 🌳 랜덤 포레스트 (Random Forest)

대표적인 결정 트리 기반의 앙상블 학습 방법. 

결정 트리를 랜덤하게 만들어 결정 트리(나무)의 **숲**을 만든다. 그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다. 

각 노드를 분할할 때 전체 특성 중 일부 특성을 무작위로 고른 다음 이 중 최선의 분할을 찾는다. 

분류 모델인 `RandomForestClassifier`는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다. 

회귀 모델인 `RandomForestRegressor`는 전체 특성을 사용한다. 

랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다. 

`RandomForestClassifier`에는 부트스트랩 샘플에 포함되지 않고 남는 OOB(out of bag) 샘플이 있다. 이 샘플을 사용하여 자체적으로 모델을 평가하는 점수를 얻을 수 있다.

**부트스트랩 샘플 (bootstrap sample)**

데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식. 

훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다. 이때 한 샘플이 중복되어 추출될 수도 있다. 기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다. 

### 👽 엑스트라 트리 (Extra Trees)

랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않는다. 즉, 각 결정 트리를 만들 때 전체 훈련 세트를 사용한다. 

대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다. 

하나의 결정 트리에서 특성을 무작위로 분할하면 성능이 낮아지겠지만, 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다. 

엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야 하지만, 랜덤하게 노드를 분할하기 때문에 계산 속도가 좀 더 빠르다. 

### 🥗 그레디언트 부스팅 (Gradient boosting)

랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법. 

훈련 속도가 조금 느리지만 더 좋은 성능을 기대할 수 있다. 

깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법.

깊이가 얕은 결정트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다. 

경사 하강법을 사용하여 트리를 앙상블에 추가한다. 

분류에서는 로지스틱 손실 함수, 회귀에서는 평균 제곱 오차 함수를 사용한다. 

### 📊 히스토그램 기반 그레디언트 부스팅 (Histogram-based Gradient Boosting)

정형 데이터를 다루는 머신러닝 알고리즘 중 가장 인기가 높은 알고리즘. 

그레디언트 부스팅의 속도를 개선하여 안정적인 결과와 높은 성능을 제공한다. 

먼저 입력 특성을 256개의 구간으로 나눈다. 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다. 이 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다. 

대표적으로 XGBoost와 LightGBM 라이브러리가 있다. 

[동작 원리]

1. 데이터의 특성을 이해하기 위해 먼저 히스토그램으로 변환 → 입력 데이터를 여러 개의 구간으로 나누고, 각 구간에 속하는 데이터 포인트의 개수를 기록
2. 초기에는 단 하나의 트리로 시작하며, 이 트리는 모든 데이터를 하나의 잎(leaf)에 할당
3. 그 다음, 그래디언트 부스팅의 핵심 원리인 그래디언트 부스팅 기준에 따라 새로운 트리를 추가 → 히스토그램을 이용하여 데이터의 분포를 고려하며, 각 구간의 예측값을 조정해 나가는 방식으로 트리를 구성
4. 트리의 구성이 완료되면, 새로운 트리에 의한 예측값과 실제값 사이의 오차(residual)를 계산 → 이 오차를 이용하여 다음 트리를 학습시키는 과정을 반복
5. 모든 트리를 학습한 후 각 트리의 예측값을 결합하여 최종 예측값을 얻음 → 오차 학습 트리가 연속적으로 생기는 구조

참고자료 :

https://dodonam.tistory.com/433

### 👩‍💻 scikit-learn

- `RandomForestClassifier` : 랜덤 포레스트 분류 클래스
- `ExtraTreesClassifier` : 엑스트라 트리 분류 클래스
- `GradientBoostingClassifier` : 그레디언트 부스팅 분류 클래스
- `HistGradientBoostingClassifier` : 히스토그램 기반 그레디언트 부스팅 분류 클래스
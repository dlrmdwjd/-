# 2024 08 28 수요일

## 특성 공학과 규제

### 다중 회귀 (multiple regression)

여러 개의 특성을 사용한 선형 회귀

**특성 공학** : 기존의 특성을 사용하여 새로운 특성을 뽑아내는 작업 

예시) 농어 길이 * 농어 높이 를 통해 새로운 특성으로 만들기

### Scikit learn Transformers

**변환기(Transformers)** : 특성을 만들거나, 전처리하기 위한 다양한 클래스

fit() : 새롭게 만들 특성 조합을 찾음

transform() : 실제로 데이터를 변환

다중회귀에서 특성의 갯수가 늘어나면 선형 모델은 아주 강력해진다! 그러나 이런 모델은 Training set에 너무 과대적합 되기 때문에 Test set에 대하여 점수가 굉장히 낮을 수 있다. 

### 규제

**규제** (Regularization) : 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방을 놓는 것. (과대적합을 막기 위함), 선형 회귀의 경우 특성에 곱해지는 계수를 작게 만드는 일.

선형 회귀 모델에 규제를 추가한 모델을 릿지 (ridge)와 라쏘 (lasso)라고 한다.

**ridge** : 계수를 제곱한 값을 기준으로 규제 적용, 선형 모델의 계수를 가능한 작게 만들어 계수 값을 0에 가깝게 하여 과대적합을 완화 (L2 정규화)

**Lasso** : 계수의 절댓값을 기준으로 규제 적용, 선형 모델의 계수를 가능한 작게 만들어 계수 값을 아예 0으로도 만들 수 있음 (L1 정규화)

**하이퍼파라미터** : 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터 (지난번의 모델 파라미터와 구분해서 기억!) 릿지 모델에서의 알파 값은 사전에 사람이 지정해주어야하는 하이퍼 파라미터.

적절한 Alpha 값을 찾기 위해서는 R-square 그래프를 그려보기. Training set과 Test set의 점수가 가까운 지점이 최적의 Alpha값.

🏞️ **추가 내용**

딥러닝 모델을 과적합 방지하는 방법으로는 크게 3가지가 있습니다.

(1) batch normalization (배치정규화)

(2) weight normalization (정규화)

(3) dropout (신경망의 뉴런을 부분적으로 생략)

(2) 정규화가 우리가 기본적으로 알고 있는  L1, L2 normalization

L1 정규화 + L1 loss 함수

![image.png](2024%2008%2028%20%E1%84%89%E1%85%AE%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%B5%E1%86%AF%20f0d84e3afbc7489da6bdd1bfea1c2b01/image.png)

![image.png](2024%2008%2028%20%E1%84%89%E1%85%AE%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%B5%E1%86%AF%20f0d84e3afbc7489da6bdd1bfea1c2b01/image%201.png)

L1 정규화의 경우 미분이 불가능하고, λ 값 (학습률) 이 작을 수록 정규화의 효과가 사라짐. 

L2 정규화 + L2 loss 함수

![image.png](2024%2008%2028%20%E1%84%89%E1%85%AE%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%B5%E1%86%AF%20f0d84e3afbc7489da6bdd1bfea1c2b01/image%202.png)

![image.png](2024%2008%2028%20%E1%84%89%E1%85%AE%E1%84%8B%E1%85%AD%E1%84%8B%E1%85%B5%E1%86%AF%20f0d84e3afbc7489da6bdd1bfea1c2b01/image%203.png)

Least Square Error에 가중치의 제곱을 더하여 가중치가 너무 크지 않도록 학습되게 함. 

L1보다 L2가 outlier에 민감한 이유는 단순히 가중치의 제곱을 더하기 때문인데요. 이럴 때 L1이 L2보다 outlier에 대하여 Robust하다고 표현할 수 있습니다.

(참고 : [https://light-tree.tistory.com/125](https://light-tree.tistory.com/125))

##
# 3-3. 특성 공학과 규제

> 선형 회귀는 특성이 많을수록 높은 효과를 낸다! ✨
> 

### 🎶 다중 회귀 (multiple regression)

여러 개의 특성을 사용한 선형 회귀

다중 회귀 모델을 훈련하는 것은 선형 회귀 모델을 훈련하는 것과 같다. 다만 여러 개의 특성을 사용하여 선형 회귀를 수행하는 것. 

### 🪶 특성 공학 (feature engineering)

기존의 특성을 사용해 새로운 특성을 뽑아내는 작업

예) 특성을 제곱하거나 특성끼리 곱해서 새로운 특성을 추가

### 🐯 규제 (regulation)

머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것. 

과대적합을 방지하기 위한 방법.

선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일이다. 

선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 많이 다르면 공정하게 제어되지 않으므로, 규제를 적용하기 전에 먼저 정규화를 해야 한다. 

> 선형 회귀 모델에 규제를 추가한 모델을 **릿지(ridge)**와 **라쏘(lasso)**라고 부른다! ✨
> 

### 🥣 릿지 회귀

선형 회귀 모델의 계수를 작게 만들어 과대적합을 완화시킨다. 

- **alpha** : 규제의 강도를 조절하는 매개변수 (하이퍼파라미터)
    - alpha 값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고 조금 더 과소적합되도록 유도한다.
    - alpha 값이 작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지므로 과대적합될 가능성이 크다.

적절한 alpha 값을 찾는 방법은 alpha 값에 대한 R^2 값의 그래프를 그려 보는 것이다. 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha 값이 된다. 

**** 하이퍼파라미터 (hyperparameter)**

모델이 학습하는 값이 아닌, 사전에 우리가 지정해야 하는 파라미터

### 👓 라쏘 회귀

라쏘 모델도 alpha 매개변수로 규제의 강도를 조절할 수 있다. 

훈련 방법 역시 릿지와 매우 비슷하다.

릿지와 달리 계수 값을 아예 0으로 만들 수도 있다. 

### 👩‍💻 scikit-learn

**PolynomialFeatures** 

사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공하며, 이러한 클래스를 변환기(transformer)라고 부른다. 

주어진 특성을 조합하여 새로운 특성을 만든다.

변환기는 입력 데이터를 변환하는 데 타깃 데이터가 필요하지 않다. 

기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가한다. 

- `fit()` : 새롭게 만들 특성 조합을 찾는다.
- `transform()` : 실제로 데이터를 변환한다.
- `include_bias` : 자동으로 절편(1)을 추가, `False`로 설정하면 절편을 포함하지 않는다. (그러나 지정하지 않아도 사이킷런 모델은 자동으로 특성에 추가된 절편 항을 무시한다.) (기본값 : `True`)
- `get_feature_names_out()` : 각각의 특성이 어떤 입력의 조합으로 만들어졌는지 알려준다.
- `degree` : 필요한 고차항의 최대 차수를 지정하는 매개변수 (기본값 : 2)
    - ❗특성의 개수를 크게 늘리면 모델이 훈련 세트에 과대적합된다.
- `interaction_only` : `True`이면 거듭제곱 항은 제외되고 특성 간의 곱셈 항만 추가된다. (기본값 : `False`)

**StandardScaler**

정규화를 위해 사용되는 클래스 (변환기)

훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야 한다. 

**Ridge**

규제가 있는 회귀 알고리즘인 릿지 회귀 모델을 훈련한다. 

- `alpha` : 규제의 강도를 조절
- `solver` : 최적의 모델을 찾기 위한 방법을 지정 (기본값 : auto)
    - `sag` : 확률적 평균 경사 하강법 알고리즘, 특성과 샘플 수가 많을 때에 성능이 빠르고 좋다.
    - `saga` : sag의 개선 버전

**Lasso**

규제가 있는 회귀 알고리즘인 라쏘 회귀 모델을 훈련한다. 

최적의 모델을 찾기 위해 좌표축을 따라 최적화를 수행해가는 좌표 하강법(coordinate descent)를 사용한다. 

- `alpha`
- `random_state`
- `max_iter` : 알고리즘의 수행 반복 횟수를 지정 (기본값 : 1000)

- **➕ 릿지와 라쏘**
    
    
    1. **규제**
    
    모델이 과대 적합을 피하여 일반화 성능을 잃지 않도록 가중치를 제한하는 방법
    
    과대 적합 완화를 위한 대표적인 방법
    
    1. **정규화**
    
    찾아야 하는 모수인 β에 제약을 줌으로써 모델을 정돈해주어 과적합이 아닌 일반성을 띄게 해준다. 
    
    ![image.png](3-3%20%E1%84%90%E1%85%B3%E1%86%A8%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%A8%E1%84%80%E1%85%AA%20%E1%84%80%E1%85%B2%E1%84%8C%E1%85%A6%209769a6d6d9424170ba609462395b9e99/image.png)
    
    Training accuracy만 있으면 최소제곱법과 같지만, generalization accuracy가 추가되면서 β에 제약을 줄 수 있어 정규화가 가능해진다. 
    
    이렇게 계수 추정치를 줄여주는 정규화 방법을 수축법(shrinkage methods)라고 한다. 
    
    1. **수축법 (shrinkage methods)**
    
    계수 추정치를 0으로 수축하는 기법
    
    회귀 계수를 0으로 수축하는 방법 → 릿지, 라쏘
    
    1. **릿지 회귀 (L2)**
    
    ![image.png](3-3%20%E1%84%90%E1%85%B3%E1%86%A8%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%A8%E1%84%80%E1%85%AA%20%E1%84%80%E1%85%B2%E1%84%8C%E1%85%A6%209769a6d6d9424170ba609462395b9e99/image%201.png)
    
    릿지 회귀의 패널티항은 파라미터의 제곱을 더해준 것이다. 이것은 미분 가능해 gradient descent 최적화가 가능하고, 파라미터의 크기가 작은 것보다 큰것을 더 빠른 속도로 줄어준다. 다시 말하면 λ가 클수록 릿지 회귀의 계수는 0에 가까워진다. (λ=0일 때, 패널티항은 효과가 없음)
    
    릿지의 경우 어느 정도 상관성을 가지는 변수들에 대해서 적절한 가중치 배분을 하게 된다. 
    
    1. **라쏘 회귀 (L1)** 
    
    ![image.png](3-3%20%E1%84%90%E1%85%B3%E1%86%A8%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%A8%E1%84%80%E1%85%AA%20%E1%84%80%E1%85%B2%E1%84%8C%E1%85%A6%209769a6d6d9424170ba609462395b9e99/image%202.png)
    
    라쏘는 패널티항에 절댓값의 합을 준다. 몇몇 유의미하지 않은 변수들에 대해 계수를 0에 가깝게 (혹은 0으로) 추정해 feature selection의 효과를 가져온다. 라쏘는 파라미터의 크기에 관계 없이 같은 수준의 정규화를 적용하기 때문에 작은 값의 파라미터를 0으로 만들어 해당 변수를 모델에서 삭제하고, 따라서 모델을 단순하게 만들어주고 해석을 용이하게 된다. 
    
    참고자료 :
    
    [1] https://hye-z.tistory.com/22
    
    [2] https://yhyun225.tistory.com/18
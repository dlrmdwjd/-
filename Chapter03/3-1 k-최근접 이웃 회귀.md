# 3-1. k-최근접 이웃 회귀

### 🌀 k-최근접 이웃 회귀 알고리즘

**지도 학습 알고리즘**은 크게 아래 두 가지로 나뉜다. 

- **분류**
- **회귀**(regression) : 임의의 어떤 숫자를 예측하는 문제, 두 변수 사이의 상관관계를 분석하는 방법
    
    예) 경제 성장률 예측, 배달 도착 시간 예측 등
    

**k-최근접 이웃 알고리즘**은 분류 뿐 아니라 회귀에도 적용이 가능하다. 

- **k-최근접 이웃 분류** : 예측하려는 샘플에 가장 가까운 샘플 k개를 선택, 샘플 클래스들 중 다수 클래스를 새로운 샘플의 클래스로 예측
- **k-최근접 이웃 회귀** : 예측하려는 샘플에 가장 가까운 샘플 k개를 선택, 이웃 샘플의 수치를 사용해 새로운 샘플의 타깃을 예측 → 샘플들의 **평균** 구하기!

**`KNeighborsRegressor`** : 사이킷런에서 k-최근접 이웃 회귀 알고리즘을 구현한 클래스

### 🐾 결정계수(R^2)

회귀의 경우 **결정계수**(coefficient of determination)이라는 점수로 모델을 평가한다. (1에 가까울수록 좋다.)

$$
R^2 = 1 - (타깃-예측)^2의 합 / (타깃-평균)^2의 합
$$

각 샘플의 타깃과 예측한 값의 차이를 제곱하여 더하고, 타깃과 타깃 평균의 차이를 제곱하여 더한 값으로 나눈다. 만약 타깃의 평균 정도를 예측하는 수준이라면 (분자와 분모가 비슷해져) R^2는 0에 가까워지고, 예측이 타깃에 아주 가까워지면 (분자가 0에 가까워지기 때문에) 1에 가까운 값이 된다. 

### ⛈️ 과대적합 vs 과소적합

**과대적합(overfitting)**

훈련 세트에서는 점수가 굉장히 좋았는데, 테스트 세트에서는 점수가 굉장히 나쁘다면 모델이 훈련 세트에 **과대적합** 되었다고 말한다. 즉, 훈련 세트에만 잘 맞는 모델이라 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않을 것이다. 

→ **해결방법** : 모델을 덜 복잡하게 만들어야 한다. 

**k-최근접 이웃 알고리즘**의 경우 **k 값을 늘린다**. 

**과소적합(underfitting)**

훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮은 경우는 모델이 훈련 세트에 **과소적합** 되었다고 말합니다. 즉, 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우이다. 

→ **해결 방법** : 모델을 조금 더 복잡하게 만들면 된다. 

**k-최근접 이웃 알고리즘**으로 모델을 더 복잡하게 만드는 방법은 **이웃의 개수를 줄이는 것**이다. 이웃의 개수를 줄이면 훈련 세트에 있는 국지적인 패턴에 민감해지고, 이웃의 개수를 늘리면 데이터 전반에 있는 일반적인 패턴을 따를 것이다. 

- **➕ [추가 조사] 회귀 모델 성능 평가 지표**
    1. **MAE (Mean Absolute Error) = 평균 절대 오차**
    
    실제 정답 값과 예측 값의 차이를 절댓값으로 변환한 뒤 합산하여 평균을 구한다. 
    
    특이값이 많은 경우에 주로 사용, 값이 낮을수록 좋다. 
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image.png)
    
    - 장점 : 직관적, 정답 및 예측값과 같은 단위를 가짐
    - 단점 :
        - 실제 정답보다 낮게 예측했는지, 높게 예측했는지 파악하기 힘듦.
        - 스케일 의존적(scale dependency)
    
    1. **MSE (Mean Squared Error) = 평균 제곱 오차**
    
    실제 정답 값과 예측 값의 차이를 제곱한 뒤 평균을 구한다. 
    
    값이 낮을수록 좋다.
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%201.png)
    
    - 장점 : 직관적
    - 단점 :
        - 제곱하기 때문에 1 미만의 에러는 작아지고 그 이상의 에러는 커진다.
        - 실제 정답보다 낮게 예측했는지, 높게 예측했는지 파악하기 힘듦.
        - 스케일 의존적(scale dependency)
    
    1. **RMSE (Root Mean Squared Error) = 평균 제곱근 오차**
    
    MSE에 루트를 씌워서 에러를 제곱해서 생기는 값의 왜곡을 줄인다.
    
    값이 낮을수록 좋다.
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%202.png)
    
    - 장점 : 직관적
    - 단점 :
        - 제곱하기 때문에 1 미만의 에러는 작아지고 그 이상의 에러는 커진다.
        - 실제 정답보다 낮게 예측했는지, 높게 예측했는지 파악하기 힘듦.
        - 스케일 의존적(scale dependency)
    
    1. **MAPE (Mean Absolute Percentage Error) = 평균 절대 비율 오차**
    
    MAE를 비율, 퍼센트로 표현하여 스케일 의존적 에러의 문제점을 개선한다.
    
    값이 낮을수록 좋다.
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%203.png)
    
    - 장점 : 직관적, 다른 모델과 에러율 비교가 쉬움
    - 단점 :
        - 실제 정답보다 낮게 예측했는지, 높게 예측했는지 파악하기 힘듦.
        - 실제 정답이 1보다 작을 경우, 무한대의 값으로 수렴할 수 있다.
    
    1. **MPE (Mean Percentage Error)**
    
    MAPE에서 절댓값을 제외하여 계산한다. 
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%204.png)
    
    모델이 underperformance인지 overperformance인지 판단할 수 있다. (음수이면 overperformance, 양수이면 underperformance)
    
    1. **R2 score = R squared**
    
    상대적인 성능을 나타내어 비교가 쉽다. 실제값의 분산 대비 예측값의 분산 비율을 의미한다.
    
    1에 가까울수록 좋다.
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%205.png)
    
    다른 지표(MAE, MSE, RMSE)들은 모델마다 값이 다르기 때문에 절대 값만 보고 성능을 판단하기 어렵다.
    
    참고자료 :
    
    [1] https://white-joy.tistory.com/10
    

- **➕ [추가 조사] 과대적합, 과소적합**
    
    ### ✅ 과대적합
    
    학습 데이터셋에 지나치게 최적화여 발생하는 문제
    
    즉, 모델을 지나치게 복잡하게 학습하여 학습 데이터셋에는 모델 성능이 높게 나타나지만 정작 새로운 데이터가 주어졌을 때 정확한 예측/분류를 수행하지 못한다. 
    
    ### ✅ 과소적합
    
    머신러닝 모델이 충분히 복잡하지 않아 (최적화가 제대로 수행되지 않아) 학습 데이터의 구조/패턴을 정확히 반영하지 못하는 문제
    
    ### ✅ 탐지 방법
    
    1. **분산(variance)과 편향(bias) 기반 탐지**
    
    분산 : 데이터셋 내 데이터가 얼마나 퍼져 있는지를 나타내는 척도
    
    편향 : 데이터가 타깃으로부터 떨어져 있는 정도
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%206.png)
    
    → 우측 상단의 과녁은 데이터가 타깃 근처에 잘 모여서 분포해 있으므로 최적의 솔루션
    
    → 죄측 상단의 과녁은 데이터가 타깃에 어느 정도 모여 있어 편향이 낮지만 데이터가 많이 흩어져 분산이 높은 경우로, 과대적합이라고 한다.
    
    → 우측 하단의 과녁은 데이터가 잘 모여 있어 분산은 낮지만 데이터들이 타깃과는 거리가 멀어 편향이 높은 경우로, 과소적합이라고 한다. 
    
    ** 편향과 분산 트레이드 오프 (Bias-Variance Tradeoff)
    
    단순한 모델에서 발생하는 편향을 줄이기 위해 모델의 복잡도를 늘리면 분산이 늘어나고, 복잡한 모델에서 분산을 줄이기 위해 모델을 단순화하면 편향이 늘어나게 도니다. 
    
    1. **산점도 그래프 기반 구분**
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%207.png)
    
    → 일부 오분류된 클래스의 데이터는 잡음(nosie)로 간주하되 나머지 데이터를 정확하게 분류할 수 있어야 한다. 
    
    1. **모델 복잡도 및 손실함수 기반 탐지**
    
    손실함수 : 데이터의 실제값과 학습한 모델을 기반으로 추정한 예측값 간의 오차 (Error)
    
    복잡도 : 선형에서 비선형 모델로 갈수록 복잡도가 증가한다. (모델의 파라미터 수 증가)
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%208.png)
    
    → 모델 복잡도와 손실함수에 따라 과소적합과 과대적합이 발생하는 구간을 확인할 수 있다. 
    
    → x축은 모델의 복잡도이며 단순히 epoch 수가 아니다. 
    
    - 과소적합 발생 구간
        - 학습 데이터의 특성을 제대로 반영하기 어렵기 때문에 train loss가 높게 나타난다.
        - 모델의 학습이 잘 수행된다는 가정 하에 일정 수준 전까지는 모델이 점차 복잡해질수록 train loss와 validation loss가 함께 감소하지만, 연구자가 원하는 모델의 성능에 도달하기 전까지는 과소적합이 발생했다고 말한다.
    - 과대적합 발생 구간
        - 모델이 점차 복잡해지면 train dataset을 중심으로 모델이 학습되기 때문에 train loss는 꾸준히 감소한다.
        - 모델 복잡도가 일정 수준을 넘어감에 따라 모델이 train dataset에 과대적합 되면 오히려 validation loss가 증가하게 된다.
        - 즉, validation loss가 감소하다가 증가하는 구간에서 과대적합이 발생했다고 말한다.
    - 최적의 모델 적합 포인트 (best fit)
        - 과소적합과 과대적합이 발생하기 전후의 사이 구간
        - validation loss가 감소하다가 증가하기 시작하기 직전의 포인트
    
    ### ✅ 과소적합 해결 방법
    
    1. Parameter가 더 많은 복잡한 모델을 선택한다.
    2. 모델의 제약을 줄인다. (규제 하이퍼파라미터의 값 줄이기)
    
    ### ✅ 과대적합 해결 방법
    
    1. 데이터의 개수를 늘린다.
    2. Feature 개수를 줄인다.
    3. Parameter 선정을 적절히 한다.
    4. 훈련 데이터의 잡음을 줄인다.
    
    1. **학습 조기 종료 (Early Stopping)**
    
    특정 epoch 내 validation loss가 감소하지 않으면 과대적합이 발생했다고 간주하고 학습을 종료함으로써 validation loss가 가장 낮은 지점인 최적(optimal) epoch에서의 모델을 저장해 주는 기법
    
    ![image.png](3-1%20k-%E1%84%8E%E1%85%AC%E1%84%80%E1%85%B3%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%20%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AE%E1%86%BA%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%206976f1de89564fd6829e2f38e4a9af52/image%209.png)
    
    1. **검증 데이터 셋을 활용한다. (K-fold cross validation)** 
    
    일반적으로 사용되는 교차 검증 방법 중 하나로, Training set과 Validation을 여러 번 나눈 뒤 모델의 학습을 검증한다. 
    
    1. **정규화(Regularization)를 적용한다.**
    
    모델의 복잡도를 키우고, 과적합을 막는 방법론. 복잡도가 큰 모델을 정의하고, 그 중 중요한 파라미터만 학습.
    
    ** 정규화 종류 : Ridge 회귀(L2 regression), Lasso 회귀(L1 regression)
    
    참고자료 :
    
    [1] https://heytech.tistory.com/125 
    
    [2] https://bruders.tistory.com/80
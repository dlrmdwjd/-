# 1-3. 마켓과 머신러닝

### 🔑 키워드

**이진분류(binary classification)**

- 분류(classification) : 머신러닝에서 여러 개의 종류(클래스) 중 하나를 구별해 내는 문제
- 이진분류(binary classification) : 2개의 클래스 중 하나를 고르는 문제

**특성(feature)** : 데이터의 특징

**k-최근접 이웃(k-Nearest Neighbors) 알고리즘** : 

어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용

**모델** : 머신러닝 알고리즘을 구현한 프로그램, 또는 알고리즘을 (수식 등으로) 구체화하여 표현한 것

**훈련** : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정

**정확도** : 정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값.  `정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)`

### 📈 matplotlib

파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지

- **`scatter()`** : 산점도를 그리는 함수
    - 처음 2개의 매개변수로 x축 값과 y축 값을 전달
    - `c` : 색깔을 지정하는 매개변수
    - `marker` : 마커 스타일을 지정하는 매개변수

### 🤖 scikit-learn

- **`KNeighborsClassifier()`** : k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스
    - `n_neighbors` : 이웃의 개수를 지정하는 매개변수 (default : 5)
    - `p` : 거리를 재는 방법을 지정하는 매개변수 (default : 2)
        - 1 : 맨해튼 거리
        - 2 : 유클리디안 거리
    - `n_jobs` : 사용할 CPU 코어를 지정하는 매개변수 (default : 1)
        - -1 : 모든 CPU 코어를 사용
        - 이웃 간 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다.

- **`fit()`** : 사이킷런 모델을 훈련할 때 사용하는 메서드, 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달
- **`predict()`** : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드, 특성 데이터 하나만 매개변수로 받는다.
- **`score()`** : 훈련된 사이킷런 모델의 성능을 측정, 처음 두 매개변수로 특성과 정답 데이터를 전달
    - 이 메서드는 먼저 `predict()` 메서드로 예측을 수행한 다음, 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환합니다.

### 👩‍💻 소스 코드

```python

# 도미 데이터 준비
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

import matplotlib.pyplot as plt

# scatter plot
plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 빙어 데이터 준비
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

# scatter plot
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 훈련용 데이터 생성
length = bream_length+smelt_length
weight = bream_weight+smelt_weight

fish_data = [[l, w] for l, w in zip(length, weight)]

print(fish_data)

fish_target = [1]*35 + [0]*14
print(fish_target)

# k-최근접 이웃 알고리즘
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()

# 훈련
kn.fit(fish_data, fish_target)

# 성능 측정
kn.score(fish_data, fish_target)

# 활용
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.scatter(30, 600, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

kn.predict([[30, 600]])

# 저장된 데이터 확인
print(kn._fit_X)
print(kn._y)

# 이웃 개수를 49개로 변경
kn49 = KNeighborsClassifier(n_neighbors=49)

# 총 49개의 데이터 중 도미가 다수를 차지하므로 어떤 데이터를 넣어도 도미로 예측
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)

print(35/49)
```
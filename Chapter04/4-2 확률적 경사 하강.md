# 4-2. 확률적 경사 하강

### 🦘 점진적인 학습

- 점진적 학습 : 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방법 (= 온라인 학습)

→ 훈련에 사용한 데이터를 모두 유지할 필요도 없고, 앞서 학습한 생선을 까먹을 일도 없을 것

**확률적 경사 하강법(Stochastic Gradient Descent)**

대표적인 점진적 학습 알고리즘

훈련 세트를 사용해 최적의 위치로 조금씩 이동하는 알고리즘

이 때문에 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트되어도 학습을 계속 이어나갈 수 있다. 

(신경망 알고리즘은 데이터가 매우 많고 모델이 매우 복잡하기 때문에 확률적 경사 하강법을 꼭 이용한다.)

확률적 → ‘무작위하게’ 혹은 ‘랜덤하게’의 기술적인 표현

경사 → 기울기

하강법 → 내려가는 방법

- **에포크(epoch)** : 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정
    - 일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행한다.
- **미니배치 경사 하강법(minibatch gradient descent)** : 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식
- **배치 경사 하강법(batch gradient descent)** : 전체 샘플을 사용
    - 전체 데이터를 사용하기 때문에 가장 안정적이지만, 그만큼 컴퓨터 자원을 많이 사용하게 된다.

> **훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘**. 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 **미니배치 경사 하강법**이 된다. 한 번에 전체 샘플을 사용하면 **경사 하강법**이 된다.
> 

### 🤝 손실 함수 (loss function)

머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준

손실함수의 최솟값을 찾기 위해 확률적 경사 하강법을 사용한다. 

**손실 함수와 비용 함수**

- **손실 함수** : 샘플 하나에 대한 손실을 정의
- 비용 함수 : 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합

(일반적으로 엄격히 구분하지 않고 섞어서 사용)

분류에서의 손실 → 정답을 맞히지 못하는 것!

경사 하강법을 사용하기 위해서는 손실 함수가 미분 가능해야 한다. (경사면이 연속적!)

**로지스틱 손실 함수 (logistic loss function)**

*= 이진 크로스엔트로피 손실 함수 (binary cross-entropy loss function)*

연속적인 손실 함수를 만드는 방법

예측값과 타깃값을 곱한 후 음수로 바꿔주면 손실값이 나온다. 

- 양성 클래스(타깃=1)일 때 :
    
    $$
    -log(예측 확률)
    $$
    
- 음성 클래스(타깃=0)일 때 :
    
    $$
    -log(1-예측 확률)
    $$
    

(예측 확률이 0에서 멀어져 1에 가까워질수록 손실은 아주 큰 양수가 된다.)

** **크로스엔트로피 손실 함수(cross-entropy loss function)** : **다중 분류**에서 사용하는 손실 함수

**✨ 다양한 손실 함수**

- 로지스틱 손실 함수를 사용하면 로지스틱 회귀 모델이 만들어진다.
- 회귀의 손실 함수로 평균 절댓값 오차(MAE)를 사용할 수 있다. (타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값)
- 또는, 평균 제곱 오차(MSE)를 많이 사용한다. (타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 평균한 값)

> 손실 함수는 **확률적 경사 하강법이 최적화할 대상**. 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있다. **이진 분류**에는 **로지스틱 회귀(또는 이진 크로스엔트로피) 손실 함수**를 사용한다. **다중 분류**에는 **크로스엔트로피 손실 함수**를 사용한다. **회귀** 문제에는 **평균 제곱 오차 손실 함수**를 사용한다.
> 

### 🥸 SGDClassifier

사이킷런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스

- `loss` : 손실 함수의 종류를 지정
    - `hinge`: 기본값. 서포트 벡터 머신(support vector machine)이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수.
    - `log` : 로지스틱 회귀의 손실 함수
- `max_iter` : 수행할 에포크 횟수 (기본값 : 1000)
- `penalty` : 규제의 종류 (기본값 : `l2`, `l1`도 가능)
- `alpha` : 규제 강도 (기본값 : 0.0001)
- `ConvergenceWarning` : 모델이 충분히 수렴하지 않았다는 경고. 이런 경고가 발생한 경우 `max_iter` 값을 늘려주는 것이 좋다.
- `partial_fit()` : 점진적 학습이 가능하기 때문에 기존에 훈련한 모델을 추가로 더 훈련할 수 있다. 모델을 이어서 훈련할 때 사용하는 메서드. 호출할 때마다 1 에포크씩 이어서 훈련.
- `SGDClassifier`는 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춘다. (조기 종료)
    - `tol` : 향상될 최솟값을 지정하는 매개변수. 반복을 멈출 조건. `n_iter_no_change` 매개변수에서 지정한 에포크 동안 손실이 `tol`만큼 줄어들지 않으면 알고리즘이 중단된다. `None`으로 지정하면 자동으로 멈추지 않는다. (기본값 : 0.001)
    - `n_iter_no_change` : 기본값 5

** `SGDClassifier`는 미니배치 경사 하강법이나 배치 하강법을 제공하지 않는다.

- `SGDRegressor` : 확률적 경사 하강법을 사용한 회귀 모델
    - `loss` : 기본값 `squared_loss`

### 🐏 에포크와 과대/과소적합

확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다. 

훈련 세트 점수는 에포크가 진행될수록 꾸준히 증가하지만 테스트 세트 점수는 어느 순간(과대 적합) 감소하기 시작한다. 

- **조기 종료(early stopping)** : 과대적합이 시작하기 전에 훈련을 멈추는 것

> 에포크는 **확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복**을 의미한다. 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 **반복**한다.
> 

---

### ➕ 손실 함수와 경사하강법

1. **손실 함수 (loss function)**

머신러닝은 타깃과 예측값의 오차를 최소화하는 방향으로 모델의 파라미터들을 조정하여 가장 알맞은 모델을 찾아내는 것

오차를 정의하는 방법 : MSE, RMSE, Cross-entropy 등 다양한 함수들이 존재

머신러닝에서는 오차를 **손실(Loss)** 또는 **비용(Cost), 오차(Error)**라고 한다. 

오차를 정의한 함수는 **손실 함수(Loss function)** 또는 **비용 함수(Cost function), 오차 함수(Error function)**라고 한다. 

머신러닝 알고리즘은 **손실 함수의 값을 최소화하는 방향으로 학습하는 것이 목적**이다.

1. **경사하강법 (Gradient Descent)**

머신러닝 모델의 **옵티마이저(Optimizer)**의 한 종류

옵티마이저 : 주어진 데이터에 맞게 **모델 파라미터들을 최적화**시켜주는 역할. 즉, 손실 함수의 값이 최대한 작아지도록 모델 파라미터들을 조정.

경사하강법은 기울기를 이용하여 손실 함수의 값을 최소화한다. 

한 가지 주의할 점은 우리가 조정하고자 하는 값(변수)은 가중치(weight, w)와 바이어스(bias, b)이다. 따라서 손실 함수를 w와 b에 관한 함수로 생각을 해야 편하다. 

 

![image.png](4-2%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%80%E1%85%A7%E1%86%BC%E1%84%89%E1%85%A1%20%E1%84%92%E1%85%A1%E1%84%80%E1%85%A1%E1%86%BC%208651fca642db41b3935d1ab55a1630a3/image.png)

위의 손실 함수는 w=m에서 가장 작은 값을 갖는다.

![image.png](4-2%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%80%E1%85%A7%E1%86%BC%E1%84%89%E1%85%A1%20%E1%84%92%E1%85%A1%E1%84%80%E1%85%A1%E1%86%BC%208651fca642db41b3935d1ab55a1630a3/image%201.png)

w 값에서 손실 함수의 미분 계수가 음이라면 w를 양의 방향으로, 손실 함수의 미분 계수가 양이라면 w를 음의 방향으로 이동시켜 주어야 한다. 

경사하강법에서는 다음과 같은 식으로 w 값을 조정해준다. (w : weight / α : learning-rate)

$$
w = w - α * ∂L/∂w
$$

이런 방식을 여러 번 반복하여 최종적으로 w 값을 손실 함수가 최솟값을 갖도록 조정해주는 것이 경사하강법이다. 

학습률(Learning rate)은 w 값이 움직이는 거리를 조정해주기 위한 파라미터이다. 

옵티마이저는 가중치 외에도 bias를 조정한다. bias 또한 같은 방식으로 조정한다. 

$$
b = b - α * ∂L/∂w
$$

*❓ 손실 함수를 그냥 미분해서 극솟값을 찾지 않는 이유?*

대부분의 모델은 매우 복잡하여 많은 가중치를 가지고 있다. 따라서 필연적으로 손실 함수 또한 매우 복잡한 형태가 된다. 이런 많은 경우 손실 함수들이 닫힌 형태(Closed form)가 아니거나 도함수를 구하는 계산이 복잡하고 불가능하다. 따라서 경사하강법의 방법대로 기울기를 따라 손실을 줄여나가는 방법이 컴퓨터의 자원적인 소모를 막고, 좀 더 안전한 방법이다. 

1. **경사하강법의 한계**

- 많은 연산량과 컴퓨터 자원 소모

데이터 하나가 모델을 지날 때마다 모든 가중치를 한 번씩 업데이트 → 상당한 연산량과 컴퓨터적 자원을 요구

→ 이를 완화하기 위한 방법이 확률적 경사하강법(SGD)와 미니배치 경사하강법!

- Local Minimal (지역 극솟값) 문제

지역 극솟값으로 수렴하는 문제가 발생할 수 있다. 

- Plateau 현상 (고원 현상)

기울기가 평탄한 영역에 대해서는 기울기가 0에 수렴하여 결국 가중치가 업데이트되지 못하고 전역 최솟값을 갖는 지점이 아닌 점에서 정지해버릴 수 있다. 

- Oscillation (진동) 문제

만약 가중치들이 손실 함수에 미치는 영향이 크게 상이하다면 진동하며 최적값에 느리게 수렴하게 된다. 이는 가중치가 불안정하게 수렴한다는 것이고, 만약 이동 거리가 크다면 최적값에 수렴하지 못하고 발산해버릴 위험이 있다. (진동폭 역시 옵티마이저의 중요한 고려 사항)

1. **확률적 경사 하강법 (SGD)**

- batch : 모델의 가중치들을 한 번 업데이트시킬 때 사용하는 데이터들의 집합
- batch size : 모델의 가중치들을 한 번 업데이트시킬 때 사용하는 데이터의 개수
- epoch : 모델이 전체 데이터를 모두 사용하여 학습을 한 횟수
- iteration : 1 epoch에 필요한 batch의 개수

즉, 2500의 dataset을 크기가 100인 dataset 25개로 나누어 학습을 진행할 때, batch size가 100인 batch 25개가 생성되어 1 epoch당 25번의 iteration이 생긴다. 

** 경사하강법은 한 번의 가중치 업데이트에 모든 데이터를 사용하므로 배치 사이즈가 전체 데이터 개수. ( = 배치 경사하강법 BGD)

**[확률적 경사 하강법]**

확률적으로(랜덤하게) 데이터를 뽑아 한 번의 반복 당 한 개의 데이터를 사용하여 가중치들을 업데이트하는 방법. 즉, 배치의 크기가 1이다. 

장점 : 연산량이 비교적 매우 적어 손실 함수의 최적값에 빠른 수렴 속도를 보이고 메모리 소비량이 적다.

단점 : 한 번의 반복에 한 번의 데이터만 사용하기 때문에 최적값에 수렴하지 않을 가능성이 있어 수렴 안정성이 낮고, 진폭이 매우 크다. 

1. **미니배치 경사하강법 (MSGD)**

BGD와 SGD의 절충안.

배치의 크기를 사용자가 정하여 사용하는 방법. 정해진 배치의 크기만큼씩 데이터를 여러 묶음으로 나누어 각 묶음에 대하여 경사하강법을 적용한다. 

BGD보다는 연산량이 적고, SGD보다는 안정적으로 수렴한다. 

가장 자주 사용되는 옵티마이저. 머신러닝에서 SGD를 이용한다!라고 하면 대부분 미니배치 경사하강법을 사용한다고 이해하면 된다. 

참고자료 :

[1] https://yhyun225.tistory.com/5

[2] https://yhyun225.tistory.com/6

[3] https://yhyun225.tistory.com/7